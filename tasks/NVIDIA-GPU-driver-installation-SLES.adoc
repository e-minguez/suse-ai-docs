[#nvidia-gpu-driver-installation-sles]
= Installing {nvidia} GPU drivers on {sles}

[#nvidia-gpu-requirements-sles]
== Requirements

If you are following this guide, it assumes that you have the following
already available:

* At least one host with {slsa} {sles-ai-version} installed, physical or
  virtual.
* Your hosts are attached to a subscription as this is required for
  package access.
* A link:https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus[compatible {nvidia} GPU] installed or fully passed through to the virtual
  machine in which {slsa} is running.
* Access to the {rootuser} userâ€”these instructions assume you are
  the {rootuser} user, and not escalating your privileges via `sudo`.

[#nvidia-gpu-pre-install-sles]
== Considerations before the installation

[#nvidia-gpu-install-generation-sles]
=== Select the driver generation

You must verify the driver generation for the {nvidia} GPU that your
system has. For modern GPUs, the `G06` driver is the
most common choice. Find more details in
link:https://en.opensuse.org/SDB:NVIDIA_drivers#Install[the support database].

This section details the installation of the `G06`
generation of the driver.

[#nvidia-gpu-pre-install-additional-components-sles]
=== Additional {nvidia} components

Besides the {nvidia} open-driver provided by {suse} as part of {slsa},
you might also need additional {nvidia} components. These could include
OpenGL libraries, {cuda} toolkits, command-line utilities such as
`nvidia-smi`, and container-integration components such
as nvidia-container-toolkit. Many of these components are not shipped by
{suse} as they are proprietary {nvidia} software. This section describes
how to configure additional repositories that give you access to these
components and provides examples of using these tools to achieve a fully
functional system.

[#nvidia-gpu-pre-install-procedure-sles]
== The installation procedure

ifdef::deployment_airgap[]
. On the *remote* host, run the script
  `SUSE-AI-mirror-nvidia.sh` from the air-gapped stack
  (see xref:ai-air-gap-stack[]) to download all required
  {nvidia} RPM packages to a local directory, for example:
+
[source,bash,subs="+attributes"]
----
{prompt_user}SUSE-AI-mirror-nvidia.sh \
  -p _/LOCAL_MIRROR_DIRECTORY_ \
  -l https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64 \
  https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/
----
+
After the download is complete, transfer the downloaded directory with
all its content to each GPU-enabled *local* host.
endif::[]

ifdef::deployment_standard[]
. Add a package repository from {nvidia}. This allows pulling in
  additional utilities, for example, `nvidia-smi`.
+
For the {x86-64} architecture, run:
+
[source,bash,subs="+attributes"]
----
{prompt_root}zypper ar \
  https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ \
  cuda-sle15
{prompt_root}zypper --gpg-auto-import-keys refresh
----
+
For the {arm64} architecture, run:
+
[source,bash,subs="+attributes"]
----
{prompt_root}zypper ar \
  https://developer.download.nvidia.com/compute/cuda/repos/sles15/sbsa/ \
  cuda-sle15
transactional update {prompt_root}zypper --gpg-auto-import-keys refresh
----
endif::[]

. Install the Open Kernel driver KMP and detect the driver version.
+
[source,bash,subs="+attributes"]
----
{prompt_root}zypper install -y --auto-agree-with-licenses \
  nv-prefer-signed-open-driver
{prompt_root}version=$(rpm -qa --queryformat '%\{VERSION}\n' \
  nv-prefer-signed-open-driver | cut -d "_" -f1 | sort -u | tail -n 1)
----

. You can then install the appropriate packages for additional utilities
  that are useful for testing purposes.
+
[source,bash,subs="+attributes"]
----
{prompt_root}zypper install -y --auto-agree-with-licenses \
nvidia-compute-utils-G06=$\{version} \
nvidia-persistenced=$\{version}
----

. Reboot the host to make the changes effective.
+
[source,bash,subs="+attributes"]
----
{prompt_root}reboot
----

. Log back in and run `nvidia-cli` command as {rootuser} to ensure it correctly detects the GPU and displays the GPU details.
+
[source,bash,subs="+attributes"]
----
{prompt_root}nvidia-smi
----
+
The output of this command should show you something similar to the
following output. In the example below, the system has one GPU.
+
[source,bash,subs="+attributes"]
----
Fri Aug  1 15:32:10 2025
+------------------------------------------------------------------------------+
| NVIDIA-SMI 580.82.07      Driver Version: 580.82.07    CUDA Version: 13.0    |
|------------------------------+------------------------+----------------------+
| GPU  Name      Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp Perf Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                              |                        |               MIG M. |
|==============================+========================+======================|
|   0  Tesla T4            On  |   00000000:00:1E.0 Off |                    0 |
| N/A   33C   P8   13W /   70W |       0MiB /  15360MiB |      0%      Default |
|                              |                        |                  N/A |
+------------------------------+------------------------+----------------------+

+------------------------------------------------------------------------------+
| Processes:                                                                   |
|  GPU   GI   CI        PID   Type   Process name                   GPU Memory |
|        ID   ID                                                    Usage      |
|==============================================================================|
|  No running processes found                                                  |
+------------------------------------------------------------------------------+
----
