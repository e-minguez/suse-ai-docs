[#nvidia-gpu-driver-installation-slmicro]
= Installing {nvidia} GPU drivers on {slem}

[#nvidia-gpu-requirements-slmicro]
== Requirements

If you are following this guide, it assumes that you have the following
already available:

* At least one host with {slem} {slmicro-ai-version} installed, physical
  or virtual.
* Your hosts are attached to a subscription as this is required for
  package access.
* A
  link:https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus[compatible {nvidia} GPU] installed or fully passed through to the virtual
  machine in which {slem} is running.
* Access to the {rootuser} userâ€”these instructions assume you are
  the {rootuser} user, and not escalating your privileges via `sudo`.

[#nvidia-gpu-pre-install-slmicro]
== Considerations before the installation

[#nvidia-gpu-install-generation-slmicro]
=== Select the driver generation

You must verify the driver generation for the {nvidia} GPU that your
system has. For modern GPUs, the `G06` driver is the
most common choice. Find more details in
link:https://en.opensuse.org/SDB:NVIDIA_drivers#Install[the support database].

This section details the installation of the `G06`
generation of the driver.

[#nvidia-gpu-pre-install-additional-components-slmicro]
=== Additional {nvidia} components

Besides the {nvidia} open-driver provided by {suse} as part of {slem},
you might also need additional {nvidia} components. These could include
OpenGL libraries, {cuda} toolkits, command-line utilities such as
`nvidia-smi`, and container-integration components such
as nvidia-container-toolkit. Many of these components are not shipped by
{suse} as they are proprietary {nvidia} software. This section describes
how to configure additional repositories that give you access to these
components and provides examples of using these tools to achieve a fully
functional system.

[#nvidia-gpu-pre-install-procedure-slmicro]
== The installation procedure

ifdef::deployment_airgap[]
. On the *remote* host, run the script
  `SUSE-AI-mirror-nvidia.sh` from the air-gapped stack
  (see xref:ai-air-gap-stack[]) to download all required
  {nvidia} RPM packages to a local directory, for example:
+
[source,bash,subs="+attributes"]
----
{prompt_user}SUSE-AI-mirror-nvidia.sh \
  -p /LOCAL_MIRROR_DIRECTORY \
  -l https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64 \
  https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/
----
+
After the download is complete, transfer the downloaded directory with
all its content to each GPU-enabled *local* host.
endif::[]

. On each (local) GPU-enabled host, open up a {tr-up-shell} session to create
  a new read/write snapshot of the underlying operating system so that
  we can make changes to the immutable platform.
+
[source,bash,subs="+attributes"]
----
{prompt_root}{tr-up-shell}
----

ifdef::deployment_standard[]
. When you are in the {tr-up-shell} session, add a package repository
  from {nvidia}. This allows pulling in additional utilities, for
  example, `nvidia-smi`.
+
For the {x86-64} architecture, run:
+
[source,bash,subs="+attributes"]
----
{prompt_tr-up}zypper ar \
  https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ \
  cuda-sle15
{prompt_tr-up}zypper --gpg-auto-import-keys refresh
----
+
For the {arm64} architecture, run:
+
[source,bash,subs="+attributes"]
----
{prompt_tr-up}zypper ar \
  https://developer.download.nvidia.com/compute/cuda/repos/sles15/sbsa/ \
  cuda-sle15
{prompt_tr-up}zypper --gpg-auto-import-keys refresh
----
endif::[]

ifdef::deployment_airgap[]
. On each GPU-enabled *local* host in its
  {tr-up-shell} session, add a package repository from the
  safely transferred {nvidia} RPM packages directory. This allows
  pulling in additional utilities, for example,
  `nvidia-smi`.
+
[source,bash,subs="+attributes"]
----
{prompt_tr-up}zypper ar --no-gpgcheck \
  file:///LOCAL_MIRROR_DIRECTORY \
  nvidia-local-mirror
{prompt_tr-up}zypper --gpg-auto-import-keys refresh
----
endif::[]

. Install the Open Kernel driver KMP and detect the driver version.
+
[source,bash,subs="+attributes"]
----
{prompt_tr-up}zypper install -y --auto-agree-with-licenses \
  nvidia-open-driver-G06-signed-cuda-kmp-default
{prompt_tr-up}version=$(rpm -qa --queryformat '%\{VERSION}\n' \
  nvidia-open-driver-G06-signed-cuda-kmp-default \
  | cut -d "_" -f1 | sort -u | tail -n 1)
----

. You can then install the appropriate packages for additional utilities
  that are useful for testing purposes.
+
[source,bash,subs="+attributes"]
----
{prompt_tr-up}zypper install -y --auto-agree-with-licenses \
nvidia-compute-utils-G06=$\{version} \
nvidia-persistenced=$\{version}
----

. Exit the {tr-up} session and reboot to the new snapshot that contains
  the changes you have made.
+
[source,bash,subs="+attributes"]
----
{prompt_tr-up}exit
{prompt_root}reboot
----

. After the system has rebooted, log back in and run `nvidia-cli` command as {rootuser} to ensure it correctly detects the GPU and displays the GPU details.
+
[source,bash,subs="+attributes"]
----
{prompt_root}nvidia-smi
----
+
The output of this command should show you something similar to the
following output. In the example below, the system has one GPU.
+
[source]
----
Fri Aug  1 14:53:26 2025       
+------------------------------------------------------------------------------+
| NVIDIA-SMI 580.82.07     Driver Version: 580.82.07     CUDA Version: 13.0    |
|---------------------------------+---------------------+----------------------+
| GPU  Name         Persistence-M | Bus-Id       Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf  Pwr:Usage/Cap |        Memory-Usage | GPU-Util  Compute M. |
|                                 |                     |               MIG M. |
|=================================+=====================+======================|
|   0  Tesla T4               On  |00000000:00:1E.0 Off |                    0 |
| N/A   34C    P8     10W /   70W |    0MiB /  15360MiB |      0%      Default |
|                                 |                     |                  N/A |
+---------------------------------+---------------------+----------------------+

+------------------------------------------------------------------------------+
| Processes:                                                                   |
|  GPU   GI   CI         PID   Type   Process name                  GPU Memory |
|        ID   ID                                                    Usage      |
|==============================================================================|
|  No running processes found                                                  |
+------------------------------------------------------------------------------+
----
